---
title: "Vignette of the GetFeatistics package"
author: "Gianfranco Frigerio"
date: "2025-03-06"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GF_vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

knitr::opts_chunk$set(fig.width = 5, fig.height = 5)
```


## Introduction

The **GetFeatistics** (GF) package provides several functions useful for the elaboration of metabolomics data. In this vignette you will find an example of a workflow using the functions of this package.

## Installation

You haven't installed the package yet?! Let's amend for that just by running the following code. Just be prepared that it might take some time if you update all the dependencies.

```{r, eval=FALSE}
if (!require("devtools", quietly = TRUE)) {  
  install.packages("devtools")
}

devtools::install_github("FrigerioGianfranco/GetFeatistics", dependencies = TRUE)
```

I hope everything went well! If you had any issue, please contact me.
After some testing, I noted that there might be an error during the installation if you don't have Java (please install it before!) and if your version of Java (32 bit or 64 bit) doesn't match the version of R (please, check that if the installation doesn't work!).


## Loading the package

The following line will load the package and all its dependencies in your current working environment. Let's start the fun!

```{r, message=FALSE, warning=FALSE}
library(GetFeatistics)
```


## Create mock data

In this vignette, we will use some mock data that simulate targeted and untargeted metabolomics experiments.

```{r}
create_df_examples()
```

The vignette is composed of four main chapters: **1. Targeted analyses**, **2. Non-targeted analyses**,  **3. Statistical analyses**, and **4. Miscellaneous**.



# 1. Targeted analyses

To elaborate a targeted analyses, we need to organise the data in at least two tables:

1. The first should contain the intensities of peak area. Samples in rows, analysed compounds in columns. Hopefully you can export this table from your chromatographic vendor software.
Here we will be using the _df_example_targeted_:

```{r}
df_example_targeted
```

2. We need to prepare a second table in which:
 - the first column must be identical to the first column of the first table;
 - the second column should contain the following: "blank", "curve", "qc", or "unknown".
 - the third column should have the actual known values for "curve" and "qc" samples.

Not clear? Have a taste of the _df_example_targeted_legend_:

```{r}
df_example_targeted_legend
```

3. Did I say "at least" two tables, remember? Well, even though it's not mandatory, preparing a third table is useful especially if you work with internal standards. This data frame should contain four columns:
 - in the first column, the name of the targeted molecules;
 - in the second column, the matched internal standard (or NA if there isn't an internal standard for that molecule);
 - in the third column, the weighting factor you want for the calibration curve for that molecule. I highly recommend to use a weighting factor such as **1/X**, **1/Y**, or **1/(Y^2)** for a better accuracy of low concentrated analytes, but if you still don't want, you can put "none" or NA;
 - in the fourth column, the unit of measure of the known concentration (this is only for visualizations and it's not considered for any calculation).

Yes, everything as in the _df_example_targeted_compounds_legend_:

```{r}
df_example_targeted_compounds_legend
```

Now that we have all the tables, we just need to pass them as the three arguments of the _get_targeted_elaboration_ function!

```{r}
TARGETED_EXAMPLE_RESULTS <- get_targeted_elaboration(data_intensity = df_example_targeted,
                                                     data_legend = df_example_targeted_legend,
                                                     compound_legend = df_example_targeted_compounds_legend)
```


The output of the function is a list with 5 useful elements:

- _results_concentrations_: a table with the calculated concentrations (you will need only this for the following statistical analyses)
- _results_accuracy_: a table with the calculated accuracy (%) for curve points and qc. It's good to check that most of these values are between 80% and 120%!
- _cv_internal_standards_: the relative standard deviation (%) of the intensities of internal standards.
- _compound_legend_: it's the same table provided in the third argument. I thought it was good to keep this valuable information!
- _summary_regression_models_: stuff of the fitted linear regression models such as: slope, intercept, r squared, and adjusted r squared.
- _regression_models_: a list with the all the fitted regression models.

Amazing! Let's check for example how the _results_concentrations_ table looks like:

```{r}
TARGETED_EXAMPLE_RESULTS$results_concentrations
```


Or maybe we want to see if the coefficient of determination is good enough, that numeric value is reported in the _summary_regression_models_ table:

```{r}
TARGETED_EXAMPLE_RESULTS$summary_regression_models
```

Well, they are calibration curves, right? So, better visualise them! Let's use the function _plot_calibration_curves_. It will create a list of ggplots that you can either print on R or export using the  _export_figures_ function.

```{r}
TARGETED_EXAMPLE_CALIBRATION_CURVES <- plot_calibration_curves(targeted_elaboration = TARGETED_EXAMPLE_RESULTS)

TARGETED_EXAMPLE_CALIBRATION_CURVES$molecule01
```

```{r, eval=FALSE}
export_figures(TARGETED_EXAMPLE_CALIBRATION_CURVES, exprt_fig_type = "pdf")
```

All the figures of this package are created as ggplot object, so they can be furhter modified following the ggplot2 sintax.

Last step before moving to the next chapter: let's store separately just what we will need later for the statistics. Two easy steps.

1. Collect the data frame of results of unknown samples in an object:

```{r}
TARGETED_RESULTS_FOR_STATISTICS <- filter(TARGETED_EXAMPLE_RESULTS$results_concentrations, sample_type == "unknown")
```

2. Collect the name of the variables in another distinct object:

```{r}
TARGETED_MOLECULES <- TARGETED_EXAMPLE_RESULTS$compound_legend$compounds
```


# 2. Non-targeted analyses

This package is supposed to be used after obtaining a feature table using open source tools such as XCMS and MS-Dial.
In particular, we need a table of feature intensities like this:

```{r}
df_example_feat_intensities
```

and a featINFO table, whose first column must be identical to the first column of the feature intensity table, and the other are retention times, m/z, and any other valuable information related to that feature, such as this one:

```{r}
df_example_feat_info
```

If you worked with MS-dial, you just need to export the areas or the normalized areas, and then use the functions _get_feat_table_from_MSDial_ and _get_feat_info_from_MSDial_. For example, something like that: (check the full documentation of the functions if you want to know more!)

```{r, eval=FALSE}

FEAT_TABLE_FROM_MSDIAL <- get_feat_table_from_MSDial(MSDIAL_raw_table_file_name = "FILE_AREA_EXPORTED_FROM_MSDIAL.txt",
                                                     n_last_coloums_to_delete = 2)

FEAT_INFO_FROM_MSDIAL <- get_feat_info_from_MSDial(MSDIAL_raw_table_file_name = "FILE_AREA_EXPORTED_FROM_MSDIAL.txt",
                                                   add_AnnoLevels = TRUE)
```

If you used XCMS while working within patRoon (and if you didn't, I suggest you to do it next time as patRoon is a great "docking station" tool that feature several widely used R-based algorithms, such as metFrag for example, check it out: https://github.com/rickhelmus/patRoon) you can use the functions _get_feat_table_from_patRoon_ and _get_feat_info_from_patRoon_. Below some examples (check the full documentation to learn how what to pass in the arguments):

```{r, eval=FALSE}

FEAT_TABLE_FROM_PATROON <- get_feat_table_from_patRoon(patRoon_featureGroups = _featureGroups_DATA_FRAME_FROM_patRoon_)

FEAT_INFO_FROM_PATROON <- get_feat_info_from_patRoon(patRoon_featureGroups = _featureGroups_DATA_FRAME_FROM_patRoon_,
                                                     patRoon_MFsummary = _MFsummary_DATA_FRAME_FROM_patRoon_,
                                                     MFsummary_score_columns = c("individualMoNAScore", "score"),
                                                     add_AnnoLevels = TRUE)
```

Great. The previous two chunks are not actually run, as in this vignette we will be using the mock _df_example_feat_intensities_ and _df_example_feat_info_.
If you have a list of actual chemical standards (with or without the retention times) you can check if those standards are present in your feature tables.
To do so, you need to prepare a table like _df_example_melecules_to_search_:

```{r}
df_example_melecules_to_search
```

Then, you can use the function _checkmolecules_in_feat_table_ passing:
- the data frames of feature intensity, feature INFO, and known molecules to the first three arguments, respectively;
- the name of the column of the known molecules data frames containing m/z values to _mz_to_search_ (or the third column will be taken as default);
- the accepted error value in _error_;
- "ppm" or "Da" in _error_type_ (which is refering to the _error_ value);
- TRUE to _check_rt_ if you have retention times and want to also check them;
- the name of the column of the known molecules data frames containing retention time values to _rt_to_search _ (or the second column will be taken as default);
- the accepted window of retention time to _rt_window_ (and be sure that the unit, like seconds or minutes, is consistent!)

```{r}
CHECKED_MOLECULES <- checkmolecules_in_feat_table(featmatrix = df_example_feat_intensities,
                                                  featinfo = df_example_feat_info,
                                                  molecules_list = df_example_melecules_to_search,
                                                  mz_to_search = NULL,
                                                  error = 20,
                                                  error_type = "ppm",
                                                  check_rt = TRUE,
                                                  rt_to_search = NULL,
                                                  rt_window = 12)

CHECKED_MOLECULES[,c("coumpound", "featname", "rt_confirmed", "rt", "rt_shift", "mz", "mz_shift")]
```


Amazing!!
Now, if you are a good metabolomer, you know how much is important to clean the dataset using well-established methods such as pooled quality controls (QCs).
If you want to filter the feature data table considering defined cut-offs from the analyses of pooled QCs, you might want to read the next step carefully!

First, you need to prepare a table containing the following information:

- The first column contains the sample names.
- The second column should contain "REMOVE" (to not consider at all those samples, I suggest to do so for the first analyses of each batch), "blank", "QC", "QC_half" (if you prepared some 50:50 dilution of pooled QC samples) or "unknown".
- The third should be prepared only if separated pooled QCs were prepared. If you want more information of such an approach, please check the idea behind it by reading this paper: https://doi.org/10.3390/molecules27082580.

```{r}
df_example_qc_sampletype
```


This table, together with the table of feature intensities, is all you need to run the _QCs_process_ function, which will filter out from your table the features that will not meet the cut-offs:

- _step1_: if TRUE, features with a relative standard deviation (CV%) greater than the value defined in the _step1_cutoff_ will be filtered out.
- _step2_: if TRUE, features not present in at least a percentage of QC samples as defined in _step2_cutoff_ will be filtered out.
- _step3_: if TRUE, features with a blank contribution, i.e.: the ratio between mean of blank and mean of QC, greater than the value defined in the _step3_cutoff_ will be filtered out.
- _step4_: if TRUE, features whose mean in "QC_half" samples are not between the percentage range of two values defined in _step4_cutoff_ compared to the mean of QCs will be filtered out.
- Moreover, if _sep_QC_ is TRUE, those steps will be performed separately in each QC groups, and then features that are confirmed in at least one of the groups defined in _QC_to_merge_ will be kept.

```{r}
UNTARGETED_FEAT_TABLE_QC_FILTERED <- QCs_process(featmatrix = df_example_feat_intensities,
                                                 sampletype = df_example_qc_sampletype,
                                                 sep_QC = TRUE,
                                                 QC_to_merge = c("tot", "F3A", "F3B", "F3C"),
                                                 step1 = TRUE,
                                                 step1_cutoff = 50,
                                                 step2 = TRUE,
                                                 step2_cutoff = 50,
                                                 step3 = TRUE,
                                                 step3_cutoff = 50,
                                                 step4 = FALSE,
                                                 step4_cutoff = c(20, 80),
                                                 rtrn_filtered_table = TRUE,
                                                 remove_results = TRUE,
                                                 remove_QC_and_blanks = TRUE)
```

I'd suggest also to filter the featINFO table accordingly, you could run something like this:

```{r}
UNTARGETED_FEAT_INFO_QC_FILTERED <- filter(df_example_feat_info,
                                           featname %in% UNTARGETED_FEAT_TABLE_QC_FILTERED$featname)
```


Brilliant. Last step of this chapter is to prepare what we need for the following statistical elaborations. Statistically speaking, considering the samples as observations (so as rows) and the features as variables (so as columns) is better for statistics. Luckily, I prepared a function that transpose our current feature table:

```{r}
UNTARGETED_RESULTS_FOR_STATISTICS <- transpose_feat_table(UNTARGETED_FEAT_TABLE_QC_FILTERED)
```

Finally, we also need to collect the feature names in a character vector object:

```{r}
FEATURES_CONSIDERED <- colnames(UNTARGETED_RESULTS_FOR_STATISTICS)[-1]
```


# 3. Statistical analysis

Not bad so far. Before actually start off with the statistics, let's put together the data from targeted and untargeted analyses in a single dataframe, together also with some other variables relevant for your samples (for example in an epidemiological study, we could have patient data such as age, BMI, sex, and more). In this vignette we will use mock data from the _df_example_sample_data_ which contains three categorical variables and two numerical variables.


```{r}
EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES <- df_example_sample_data %>%
  left_join(TARGETED_RESULTS_FOR_STATISTICS, by = "samples") %>%
  left_join(UNTARGETED_RESULTS_FOR_STATISTICS, by = "samples")
```


Moreover, let's put together the character vectors of the known molecule variables from the targeted analyses with the character vector of the feature names.

```{r}
ALL_POTENTIAL_COMPOUNDS <- c(TARGETED_MOLECULES, FEATURES_CONSIDERED)
``` 

Obviously, if you have only targeted or only untargeted experiments, and if you don't have any other data to join, you can skip this preparatory step and go directly on with the statistics.
 

## 3.1. Descriptive statistics 

Have you ever wished of a table that gives you quick summary of the data with mean or median, and/or with percentiles? Maybe also with the option of doing it separately for each group of a defined categorical variables (that could be for example the control/disease groups)?
Your dream will come true with the _gentab_descr_ function!
Just put the dataframe in the first argument, the character vector with the name of all the numerical variables of interest in the second argument, the name of the factor variable in the third argument.
You specify what you want in the type argument, for example "median (5th; 95th percentile)". Check the full documentation by tying _?gentab_descr_ to know the other options such as setting a defined number of digital places, or how to consider missing values.


```{r}
EXAMPLE_DESCRIPTIVE_STATISTICS_RESULTS <- gentab_descr(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES,
                                                       v = ALL_POTENTIAL_COMPOUNDS,
                                                       f = "factor_condition3lev",
                                                       type = "median (5th; 95th percentile)",
                                                       ROUND = TRUE,
                                                       dig = 3,
                                                       unit_mes = NA,
                                                       missing = "only if >0")
``` 
 
Don't forget to export it as an external table with the _export_the_table_ function and it will be ready to be used as a table of your next publication!
 
```{r, eval=FALSE}
export_the_table(EXAMPLE_DESCRIPTIVE_STATISTICS_RESULTS, exprtname = "The_results_of_descriptive_statistics", exprt_type = "xlsx")
``` 

 
## 3.2. Data normalisation

Before moving to parametric inferential statistics, we should check if the data are normally distributed. We could do it in three different ways:

1. We could plot a density plot and check if the data distribution resembles a Gaussian:

```{r}
DENSITY_PLOTS <- test_normality_density_plot(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES, v = ALL_POTENTIAL_COMPOUNDS)
DENSITY_PLOTS[[1]]
```
 
2. We could plot a Q–Q plot, anchd check that all points are within the grey area of the graph: 

```{r}
QQPLOT <- test_normality_q_q_plot(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES, v = ALL_POTENTIAL_COMPOUNDS)
QQPLOT[[1]]
```
 
3. We could apply Shapiro–Wilk tests: if the _P_-value is less than a defined cut-off (classical 0.05) means that data are not normally distributed:

```{r}
SHAPIRO_TABLE <- test_normality_Shapiro_table(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES, v = ALL_POTENTIAL_COMPOUNDS, pvalcutoff = 0.05, cutpval = FALSE)
SHAPIRO_TABLE
```


Most of the time, we need to normalise metabolomics data. We can do it in one step with the _transf_data_ function. Here a little guide to how to prepare the arguments:

- As most of the statistical function of the package, put the data frame of interest, and a character vector with the name of the numerical columns of interest in the first two arguments.
- If _missing_replace_ is TRUE, each NA in the data will be replaced by the minimum value of that variable, after being processed by an operation ("divide", "multiply", or "exponentiate") passed to _missing_repl_type_ and the value in _missing_repl_value_. For example, if you want to replace the missing values with 1/5 of the minimum value, set _missing_repl_type = "divide"_ and _missing_repl_value = 5_. If you want to replace missing values with the squared root of the minimum value, set _missing_repl_type = "exponentiate"_ and _missing_repl_value = 1/2_.
- If _log_transf_ is TRUE, the data will be log-transformed and the base of the logarithm is the value reported in _log_base_ (choose exp(1) is for the natural logarithm).
- If _scaling_ is TRUE, data will be scaled considering what is reported in _scaling_type_, in particualr:
  - "mean_scale": data are subtracted by the mean;
  - "auto_scale": data are subtracted by the mean and divided by the standard deviation;
  - "pareto_scale": data are subtracted by the mean and divided by the squared root of the standard deviation;
  - "range_scale": data are subtracted by the mean and divided by the difference between the maximum and the minimum values.
- The output will be a table with more columns containing the transformed values, to automatically get the names of those columns in separate objects saved in the global environment, you can specify _vect_names_transf_ as TRUE and the starting part of the name that object in _name_vect_names_

Hopefully running the example will make things clear:


```{r}
EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf <-  transf_data(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES,
                                                              v = ALL_POTENTIAL_COMPOUNDS,
                                                              missing_replace = TRUE,
                                                              missing_repl_type = "divide",
                                                              missing_repl_value = 5,
                                                              log_transf = TRUE,
                                                              log_base = exp(1),
                                                              scaling = TRUE,
                                                              scaling_type = "pareto_scale",
                                                              vect_names_transf = TRUE,
                                                              name_vect_names = "ALL_POTENTIAL_COMPOUNDS_transf")
```

Look at the column names of the object: "_mr" has been added for missing value replaced, "_ln" for the log-transformation, and "paretosc" for the pareto scaling:

```{r}
glimpse(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf)
```

Those name are also saved in different objects:

```{r}
ALL_POTENTIAL_COMPOUNDS_transf_mr
ALL_POTENTIAL_COMPOUNDS_transf_mr_ln
ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc

```

I decided to creat the function in a way that it explicates all the results of the different steps of the transformations, to compare and have access to the complete information in case of need.
...But then I realised the dataframe can have just too many columns!! 
To clean it and have only the data of interest, and without those wired suffix, just run the function: _clean_transf_colnames_

```{r}
EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf_cleaned <- clean_transf_colnames(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                                                               v = ALL_POTENTIAL_COMPOUNDS,
                                                                               suffix_to_consider = "_mr_ln_paretosc")
glimpse(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf_cleaned)
```

Voilà! Only what we need, transformed, and with cleaned names!

 
## 3.3. Univariate statistics with 2 groups

Let's compare differences in two groups with a t-test. A t-test for each numerical variable of interest will be performed with the following function.
Overall, we need to set the arguments in a similar way to other functions of this package:

- the full data frame in the first argument;
- a character vector with column names of the numerical variable of interest in the second argument;
- the name of the column with the categorical variable (since this is a t-test, it must be a categorical variable with two levels) in the third argument;
- _paired_ set to TRUE for a paired t-test;
- _FDR_ to TRUE for further correcting the p-vales for multiple corrections across the different features;
- if _cutPval_ is TRUE, every p-value below 0.001 will be replaced with "<0.001";
- if _groupdiff_ is TRUE, additional columns specifying which group has the higher mean will be also added to the table, only for significant differences considering the cutoff specified in _pcutoff_;
- if _filter_sign_ is TRUE, differences that have a p-value higher than _pcutoff_ will be filtered out.

```{r}
EXAMPLE_T_TEST <- gentab_P.t.test(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf_cleaned,
                                  v = ALL_POTENTIAL_COMPOUNDS,
                                  f = "factor_condition2lev",
                                  paired = FALSE,
                                  FDR = TRUE,
                                  cutPval = FALSE,
                                  groupdiff = TRUE,
                                  pcutoff = 0.05,
                                  filter_sign = FALSE)

EXAMPLE_T_TEST
```


We can also perform a Fold Change analysis!
Differently from the t-test, the Fold Change should be performed on data before the scaling (because we don't want data with negative values for the Fold Change!).
We do want to replace missing values though. While, if you performed or not already the log-transformation can be specified in the _are_log_transf_ argument (and if TRUE, the function will do fold-change substracting the data instead of the ratio. Logarithmic properties rulez!).
_second_to_first_ratio_ as TRUE implies that you want to see the effect of the group that is specified as the second level of the factor compared to the first; the opposite will happen if FALSE.

```{r}
EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_missing_repl <-  transf_data(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES,
                                                                    v = ALL_POTENTIAL_COMPOUNDS,
                                                                    missing_replace = TRUE,
                                                                    missing_repl_type = "divide",
                                                                    missing_repl_value = 5,
                                                                    log_transf = FALSE,
                                                                    scaling = FALSE,
                                                                    vect_names_transf = TRUE,
                                                                    name_vect_names = "ALL_POTENTIAL_COMPOUNDS_missing_repl")

EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_missing_repl_cleaned <- clean_transf_colnames(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_missing_repl,
                                                                                     v = ALL_POTENTIAL_COMPOUNDS,
                                                                                     suffix_to_consider = "_mr")

EXAMPLE_FC <- gentab_FC(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_missing_repl_cleaned,
                        v = ALL_POTENTIAL_COMPOUNDS,
                        f = "factor_condition2lev",
                        second_to_first_ratio = TRUE,
                        paired = FALSE,
                        are_log_transf = FALSE,
                        log_base = 2,
                        filter_sign = FALSE,
                        FCcutoff = 2)

EXAMPLE_FC
```

Why not combining the information of the t-test and the Fold Change analysis into a Volcano plot?
You can do it with the function _Volcano_ttest_FC_, that takes as input the results of the _gentab_P.t.test_ and _gentab_FC_ functions. You can leave the other arguments as default or personalise them, make sure to read the documentation with _?Volcano_ttest_FC_ first!

```{r}
Volcano_plot_ttest_FC <- Volcano_ttest_FC(ttest_results = EXAMPLE_T_TEST,
                                          FC_results = EXAMPLE_FC,
                                          FDR = TRUE,
                                          log_base = 2,
                                          cut_off_names = -log10(0.05),
                                          line1_position = -log10(0.05),
                                          line2_position = -log10(0.0001),
                                          names_to_plot = NULL,
                                          category = NULL,
                                          col_pal = NULL)
Volcano_plot_ttest_FC
```

We could do also some box-plots already, but I will describe them in the next section!

## 3.4. Univariate statistics with 3 or more groups

Very similarly to the t-test, we can do a one-way ANOVA, with posthocTukeyHSD for pairwise group comparison. The argument are similar to the _gentab_P.t.test_ function, obviously in the third one we need to put the name of a categorical variable with three or more levels:

```{r}
EXAMPLE_ANOVA_1WAY <-  gentab_P.1wayANOVA_posthocTukeyHSD(DF = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf_cleaned,
                                                           v = ALL_POTENTIAL_COMPOUNDS,
                                                           f = "factor_condition3lev",
                                                           FDR = TRUE,
                                                           groupdiff = TRUE,
                                                           pcutoff = 0.05,
                                                           filter_sign = FALSE,
                                                           cutPval = FALSE)
glimpse(EXAMPLE_ANOVA_1WAY)
```


A nice graphical visualisation of the distribution of the data can be obtained with the following function that generate some boxplots:

```{r}
EXAMPLE_BOX_PLOTS <- getBoxplots(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf_cleaned,
                                 v = ALL_POTENTIAL_COMPOUNDS,
                                 f = "factor_condition3lev")
EXAMPLE_BOX_PLOTS[[1]]
```


Since it generates a list with a box plot for each variable of interest. We could use the _export_figures_ to export them all in a single pdf file:

```{r, eval=FALSE}
export_figures(EXAMPLE_BOX_PLOTS, exprtname_figures = "All_the_amazing_boxplots", exprt_fig_type = "pdf")
```


I also prepared a cool functions that performs the Fold Change analyses for each group pair! It is similar to _gentab_FC_, and I also added an argument _only_on_positive_ that you can set to TRUE if you want to be sure the FC analysis is performed only on those positive data.

```{r}
EXAMPLE_multiple_FC <- gentab_FC_more_than2levels(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_missing_repl_cleaned,
                                                  v = ALL_POTENTIAL_COMPOUNDS,
                                                  f = "factor_condition3lev",
                                                  second_to_first_ratio = TRUE,
                                                  paired = FALSE,
                                                  are_log_transf = FALSE,
                                                  log_base = 2,
                                                  only_on_positive = FALSE)
glimpse(EXAMPLE_multiple_FC)
```




Ah! ...and why, just one-way ANOVA?? Two-ways ANOVA are also an option and can be obtained with the following function, specifying the two categorical factor in the third argument and if we want interactions in the _interact_ argument:

```{r}
EXAMPLE_ANOVA_2WAY <-  gentab_P.2wayANOVA_posthocTukeyHSD(DF = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf_cleaned,
                                                           v = ALL_POTENTIAL_COMPOUNDS,
                                                           f = c("factor_condition3lev", "factor_condition4lev"),
                                                           interact = FALSE,
                                                           FDR = TRUE,
                                                           groupdiff = TRUE,
                                                           pcutoff = 0.05,
                                                           filter_sign = FALSE,
                                                           cutPval = FALSE) 
```
 


Great, last part of this chapter: merging the statistics just obtained with the featINFO table from an untargeted study!
After all, the name of this package is GetFeatistics (get features + statistics!!)

You can do that with the _addINFO_to_table_ function:

```{r}
FINAL_FEAT_INFO_COMBINED_WITH_STATISTICS <- addINFO_to_table(df1 = EXAMPLE_ANOVA_2WAY,  
                                                             colfeat_df1 = "Dependent",
                                                             dfINFO = df_example_feat_info,
                                                             colfeat_dfINFO = "featname",
                                                             add_char_to_INFO = FALSE,
                                                             char_to_add_to_INFO = "_INFO")
```

The warnings that we got, tell us that the molecule of our mock target analyses did not find a match in the featINFO, and that's correct of course!

This is a glimpse of the content of this table:
```{r}
glimpse(FINAL_FEAT_INFO_COMBINED_WITH_STATISTICS)
``` 

Which we can finally export it this way:

```{r, eval=FALSE}
export_the_table(FINAL_FEAT_INFO_COMBINED_WITH_STATISTICS)
```


## 3.5. Principal Component Analysis, and Heat Map with Cluster Analysis

In this section, I will show you two functions for some cool multivariate data visualisation. The function might look complex, but don't be scared, I'll explain everything you need to know!
First of all, in addition to the data frame we used for other statistical analyses (df), we can optionally pass another one (dfv) containing information about the metabolites (such as molecule categories). For the purpose of this vignette we can prepare a data frame with a factor variable indicating whether the compound is a molecule from the targeted analysis or a feature from the untargeted experiment.

```{r}
COMPOUNDS_INFO <- tibble(The_compounds = ALL_POTENTIAL_COMPOUNDS,
                           type = factor(ifelse(grepl("feature", ALL_POTENTIAL_COMPOUNDS), "feat", "mol"), levels = c("feat", "mol")))
``` 


Let's look at the function for Principal Component Analysis (PCA): _getPCA_.

- arguments _df_ and _v_ are the same as we saw before (the dataframe with data and the names of columns containing the data, respectively). Remember to use normalised data!
- _s_ is optional and it's the name of the column with samples names. Pass it only if you want those names on the score plot.
- _f_, also optional, is the name of the column with sample groups. Pass it only if you want to color the dots on score plot based on sample groups.
- _dfv_, optional, is the data frame with compound information we just created.
- _sv_ and _fv_, similarly to _s_ and _f_ are optional and are the column names of the dfv data frame with names and group you want on the loading plot.
- even if you didn't have a dfv fata frame, but you want the names on the loading plot, just pass this as TRUE.
- _col_pal_ and _col_pal_fv_ can be used to specified the colors for groups. If not specified some defined colors will be used.
- Choose the two PC you want on the score and loading plots by passing them here.
- set _ellipses_on_score_ and /or _ellipses_on_loading_ if you also want to create ellipses based on the groups.

The output of such a functions is a list with four objects: the score and loading plots as ggplot objects, and the values of all principal components for the scores and loading attached to the passed dataframes. 

```{r}
PCA_list <- getPCA(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf_cleaned,
                   v = ALL_POTENTIAL_COMPOUNDS,
                   s = NULL,
                   f = "factor_condition2lev",
                   dfv = COMPOUNDS_INFO,
                   sv = "The_compounds",
                   fv = "type",
                   labels_on_loading = TRUE,
                   col_pal = NULL,
                   col_pal_fv = NULL,
                   PC_to_plot = c("PC1", "PC2"),
                   ellipses_on_score = TRUE,
                   ellipses_on_loading = FALSE)

PCA_list$df_with_scores_table[,c("samples", "PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9")]

PCA_list$dfv_with_loadings_table[,c("The_compounds", "PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9")]

PCA_list$score_plot

PCA_list$loading_plot
``` 

Great, now let's see how to do an heat map with or without cluster analysis.

- The first set of arguments are similar to the previous function, but here you can specify multuple columns for _f_ and _fv_, for example if you can divide your samples into different groups. For each column passed, colored bars will be added at the top and/or at the left of the heatmap to indicate the groups.
- In _order_df_by_ and/or _order_dfv_by_ we can put names of column to order the samples and/or features (these arguments are very useful, but will be completely ignored if you perform a cluster analysis, see below)
- By default _trnsp_ is TRUE and the features will be as rows of the heatmap. The opposite will happen if _trnsp_ is FALSE.
- _cluster_rows_ and _cluster_columns_ can be set to TRUE to perform the cluster analyses.
- _name_rows_ and/or _name_columns_, can be set to TRUE to add the labels if the arguments _s_ and/or _sv_ were defined. _rotate_name_columns_ is usefull as TRUE if you have long names and short space.
- _three_heat_colors_ needs to be a vector of three colors: the rectangles will be colored with the first color if higher, or to the last color if lower.
- if _set_heat_colors_limits_ is TRUE, the same maximum and minimum value will be set for the colors of the rectangles (also to ensure zero is always referring to the middle color). This is not really needed, though, if your data are well normalised. Those color limits can also be manually set with _heat_colors_limits_.
- _col_pal_list_ can be left to NULL to use some default colors the grouping, or a list of color can be passed. Name each element of the list as the element passed to _f_ and/or _fv_ to ensure the matching.


```{r}
HeatMap <- getHeatMap(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf_cleaned,
                      v = ALL_POTENTIAL_COMPOUNDS,
                      s = "samples",
                      f = c("factor_condition2lev", "factor_condition3lev"),
                      dfv = COMPOUNDS_INFO,
                      sv = "The_compounds",
                      fv = "type",
                      order_df_by = NULL,
                      order_dfv_by = NULL,
                      trnsp = TRUE,
                      cluster_rows = TRUE,
                      cluster_columns = TRUE,
                      name_rows = TRUE,
                      name_columns = FALSE,
                      rotate_name_columns = FALSE,
                      three_heat_colors = c("magenta", "white", "cyan"),
                      set_heat_colors_limits = FALSE,
                      heat_colors_limits = NULL,
                      col_pal_list = list(factor_condition3lev = c("magenta4", "skyblue1", "lightgreen"),
                                          type = c(mol = "pink", feat = "brown")))
HeatMap
``` 


## 3.6. Linear models
 
And now, the gem of the package and the statistical approach that I personally like the most: multiple regression linear models!!
The cool part of linear model is that they can assess the association of variables while taking into account for multiple potential confounding factors.


All in a single function, that I called: _gentab_lm_long_.
 
within the function we can distinguish three modes, specified in the _mdl_ argument:

1. "lm": linear models (with fixed effects), using the _lm_ function from the stats package.
Usually, a multivariate linear model can be built, as example, with a formula like this:
dependent_variable ~ independent_variable1 + independent_variable2 + independent_variable3

In this function, different linear models will be fitted, one for each dependent variable of interest. Usually I would consider as dependent variable each measured molecule/feature.
- As for previosu function, put the full data frame in the first argument and a character vector with all the numeric variable names to be considered as dependent variables;
- Now, pay attention to the third argument, _form_ind_: there you need to put everything you would put after the ~ of the formula. Like: "independent_variable1 + independent_variable2 + independent_variable3". They must be names of columns with numeric or factor variables in the dataframe;
- if _var_perc_ is TRUE, besides the slope, an additional column with the variation percentage will be added. The data in the dependent variables should be log-transformed and scaled to correctly do this operation. The variation percentage is calculated this way: (((base^beta)-1)*100). The base of the logaritm have to be provided in the _base_ argument (exp(1) is 2.718282... so for the natural logaritm).
- Similarly to previously described functions, _FDR_ as TRUE is to add the multiple correction to the p-value, _filter_sign_ as TRUE is to keep only results with a p-value below what we indicate in _pcutoff_ and _cutPval_ as TRUE will replace p-value<0.001 with "<0.001".


```{r}
EXAMPLE_LINEAR_MODEL <- gentab_lm_long(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                       dep = ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc,
                                       form_ind = "factor_condition2lev + factor_condition3lev + numerical_condition_a + numerical_condition_b",
                                       mdl = "lm",
                                       var_perc = TRUE,
                                       base = exp(1),
                                       FDR = TRUE,
                                       filter_sign = FALSE,
                                       pcutoff = 0.05,
                                       cutPval = FALSE)
``` 


Let's have a glimpse to the table that we got out of it:
- For each line, there are the columns _dependent_ and _independent_. Among the independents, there will be the "(Intercept)", that doesn't really have any biological significance. For categorical variables, independent will contain the name of each non-reference group. For example, the variable _factor_condition3lev_ has 3 levels: "F3A", "F3B", and "F3C"; Since "F3A" is the reference category, _independent_ will contain: "factor_condition3levF3B", which tell us about the association of the dependent variable in F3B vs F3A, and "factor_condition3levF3C", explaining what happens comparing F3C vs F3A.
- _N_observations_: is a check that tell us how many observations have been used for that linear model.
- _beta_ is the slope for that dependent and independent variables, _beta_95confint_lower_ and _beta_95confint_upper_ are the lower and upper limit of the 95% confidence interval of the beta, _SE_ is the standard error of it; and _adj_R_sqrd_ is the adjuster R squared of the model.
- _Pvalue_ and _FDR_Pvalue_ are the p-value referred to the beta being significantly different from zero.
- _negative_log10p_ and _negative_log10fdr_ are the negative log-transformed (base 10) P-values (usefull for VOlcano plots, see later) 
- _variation_perc_ is the calculated variation percentage

```{r}
glimpse(EXAMPLE_LINEAR_MODEL)
``` 


These are great data for a Volcano plot!
Considering an independent variable, a Volcano plot will allow us to see the associations with it of all the dependent variables at once:

- put the linear models table results in the first argument;
- the independent variable name in the second argument;
- in the third and fourth augment, we put what we want in the x-axis (usually "beta" or "variation_perc") and on the y-axis ("negative_log10p" or "negative_log10fdr");
- if we set _dep_cat_ as TRUE and we specify in _category_ a further column in the table with a categorization for the dependent variables (for example the class of the molecules), then the dots will be differently colored based on that categories;
- only the names of the dependent variables with an Y-valuegrater than the _cut_off_names_ will be shown (so if we only whant names of what has a P-value lower than 0.0001, we can put here -log10(0.0001));
- with _line1_ and/or _line2_ set to TRUE, additional horizontal dotted lines will be reported in the _line1_position_ and/or _line2_position_.

```{r}
EXAMPLE_VOLCANO_PLOT_1 <- Volcano_lm(tab = EXAMPLE_LINEAR_MODEL,
                                     ind_main = "numerical_condition_a",
                                     x_values = "variation_perc",
                                     y_values = "negative_log10fdr",
                                     dep_cat = FALSE,
                                     category = NULL,
                                     cut_off_names = -log10(0.0001),
                                     line1 = TRUE,
                                     line1_position = -log10(0.05),
                                     line2 = FALSE)

EXAMPLE_VOLCANO_PLOT_1
``` 

As usual, we can easily export it in this way:

```{r, eval=FALSE}
export_figures(EXAMPLE_VOLCANO_PLOT_1, exprt_fig_type = "png", plot_sizes = c(14, 14), plot_unit = "in")
``` 

Pay attention: if we want to generate a Volcano plot to appreciate the association with a categorical variable, remember that the name of the independent variables includes also the group we want to check (compared to the reference group):

```{r}
EXAMPLE_VOLCANO_PLOT_2 <- Volcano_lm(tab = EXAMPLE_LINEAR_MODEL,
                                     ind_main = "factor_condition2levF2B",
                                     x_values = "variation_perc",
                                     y_values = "negative_log10fdr",
                                     dep_cat = FALSE,
                                     category = NULL,
                                     cut_off_names = -log10(1e-04),
                                     line1 = TRUE,
                                     line1_position = -log10(0.05),
                                     line2 = FALSE)

EXAMPLE_VOLCANO_PLOT_2
``` 


2. "lmer": linear models with mixed effects (random and fixed), using the _lmer_ function from the lme4 package.
These models are particularly useful if we want to include co-variates that contain observations not independently each other (we would use a random effect in that case).
For example, in a time series analyses on same patients, the variable identifying the patients should be considered as an independent variable with random effects in this way:
dependent_variable ~ independent_variable1 + independent_variable2 + (1|variable_with_random_effects)

Just as before, everything after the "~" should be passed to the third argument of the _gentab_lm_long_ function (as multiple models will be fitted, each for each dependent variable passed in the second argument):

```{r, eval = FALSE}
EXAMPLE_MIXED_LINEAR_MODEL <- gentab_lm_long(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                             dep = ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc,
                                             form_ind = "factor_condition2lev + factor_condition3lev + numerical_condition_a + numerical_condition_b + (1|factor_condition4lev)",
                                             mdl = "lmer",
                                             var_perc = TRUE,
                                             base = exp(1),
                                             FDR = TRUE,
                                             filter_sign = FALSE,
                                             pcutoff = 0.05,
                                             cutPval = FALSE)
``` 
 

3. "tobit": TOBIT linear models, using the _tobit_ function of the AER package.
Particularly useful for the targeted analyses, Tobit models are useful to trad dependent variable as in between categorical and continuous variables.
In particular, if we have value below the Lower Limit Of Detection of our analytical method, we can pass those as left-censored values; and if we have values higher than the the Upper Limit Of Detection, we can specify them as right censored values. The _left_cens_ and _right_cens_ arguments are made for this: we can pass a named numeric vector with those values (the names of the vector must be the dependent variables).
Of course if we transformed the data, we shoudl transforme these data as well:

```{r, eval = FALSE}

LOD_molecules_ln_paretosc <- c(molecule01_mr_ln_paretosc = (log(10)-mean(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule01_mr_ln))/sqrt(sd(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule01_mr_ln)),
                               molecule02_mr_ln_paretosc = (log(100)-mean(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule02_mr_ln))/sqrt(sd(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule02_mr_ln)),
                               molecule03_mr_ln_paretosc = (log(1)-mean(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule03_mr_ln))/sqrt(sd(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule03_mr_ln)),
                               molecule04_mr_ln_paretosc = (log(10)-mean(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule04_mr_ln))/sqrt(sd(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule04_mr_ln)))

EXAMPLE_TOBIT_LINEAR_MODEL <- gentab_lm_long(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                             dep = c("molecule01_mr_ln_paretosc" "molecule02_mr_ln_paretosc" "molecule03_mr_ln_paretosc" "molecule04_mr_ln_paretosc"),
                                             form_ind = "factor_condition2lev + factor_condition3lev + numerical_condition_a + numerical_condition_b",
                                             mdl = "tobit",
                                             left_cens = LOD_molecules_ln_paretosc,
                                             right_cens = NULL,
                                             var_perc = TRUE,
                                             base = exp(1),
                                             FDR = TRUE,
                                             filter_sign = FALSE,
                                             pcutoff = 0.05,
                                             cutPval = FALSE)
``` 


## 3.7. Pathway Enrichment Analysis

Concluding the workflow of this package, a pathway enrichment analysis can be performed following the workflow of this package using the tool FELLA (https://doi.org/10.1186/s12859-018-2487-5).
I wrapped everything we need to do that in the function _do_FELLA_enrichment_analysis_:

- _organism_code_ is the three letter code of the organism. "hsa" is for human (https://www.genome.jp/kegg/tables/br08606.html).
- _KEGG_codes_ are the KEGG codes of the metabolites of interest (like the one you found statistically significant). In the following examples they are the codes of cysteine, glutamate, phenylalanine.
- _path_databases_ is were you have the FELLA database and if you don't have already, where they will be automatically created.
- The function will both creating objects in the global environment and also exporting csv table and png picture in the working directory. _output_prefix_ and _output_suffix_ can add some other characters to those object and file names.

```{r, eval = FALSE}
do_FELLA_enrichment_analysis(organism_code = "hsa",
                             KEGG_codes = c("C00097", "C00025", "C00079"),
                             path_databases = "C:/databases/FELLA/",
                             output_prefix = "SomeAA_",
                             output_suffix = "")
``` 


# 4. Miscellaneous

There are some miscellaneous functions that I prepared that can be generically useful.

Some are to adjust variable or sample names like: _zero_prefixing_ (to get some names ordered by name correctly), _fix_duplicated_, _fix_names_ (to remove special characters) and the related _check_if_fix_names_needed_.
I am also particularly proud of _get_phase_amount_ (to know the exact amount mobile phases needed for a gradient) and of _GetCombined_featTable_MSDial_ (to combine three feature intensity tables from different batches into a single one)

I would particularly speak here in more details of _merge_results_.
Suppose you perform your untargeted analyses, and then the statistical analyses, on different combinations of chromatographic runs and mass polarity (such as HILIC POS, RPLC NEG). Wouldn't you like to combine the results into a single table?
Your wish will be granted with _merge_results_:

- combine your result tables in a list, then pass it to _results_list_
- if sample names of different batches of analyses are identical, except for some string (for example: "SAMPLE_NAME1_HILIC_POS", "SAMPLE_NAME2_HILIC_POS"...), pass to _strings_in_colnames_to_remove_ what you need to remove to make sample names idendical.
- The reference column that will be used to look for duplicates (for example the compound names) should be passed to _by_column_. For example you only want to keep one of each unique compound from your different result tables.
- But how to prioritise the feature table from which taking that compound? by specifying the _if_duplicated_consider_columns_. Can be more than one, for example look the one with the lowest "PvaluesFDR", and in case of tie, look "Pvalues".
- For each column passed to _if_duplicated_consider_columns_, if you want to do the raking in decreasing order set _decreasing_ to TRUE. In the example of p-values we want to prioritise lower p-values first, so _decreasing_ should set to FALSE
- A new table will be returned with a column called _name_new_column_ with the combined results (and also printing what happpened in case of duplicated to the console!)
  
```{r, eval = FALSE}
MERGED_TABLE <- merge_results(results_list = list(HILIC_POS = _A_DF_WTIH_RESULTS_, RPLC_NEG = _ANOTHER_DF_WITH_RESULTS_),
                              strings_in_colnames_to_remove = c("HILIC_POS", "RPLC_NEG"),
                              by_column = "variables",
                              if_duplicated_consider_columns = c("PvaluesFDR", "Pvalues"),
                              decreasing = c(FALSE, FALSE),
                              name_new_column = "chromatographic_run")
``` 


## Conclusion

I hope you enjoying the GetFeatistics package!
Don't forget to check the full documentation for each function.
And don't forget to cite it if you use if for your elaborations!
Finally, thank you for reporting me any issue or bug you might encounter while using this package!
