---
title: "Vignette of the GetFeatistics package"
author: "Gianfranco Frigerio"
date: "2024-07-22"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GF_vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<img src="vignettes/GetFeatisitcs_workflowGF.png">

## Introduction

The **GetFeatistics** (GF) package provides several functions useful for the elaboration of metabolomics data. In this vignette you can find an example of a workflow using the functions of this package.

## Installation

You haven't installed the package yet?! Let's amend for that just by running the following code. Just be prepared that it might take some time if you update all the dependencies.

```{r, eval=FALSE}
if (!require("devtools", quietly = TRUE)) {  
  install.packages("devtools")
}

devtools::install_github("FrigerioGianfranco/GetFeatistics", dependencies = TRUE)
```

I hope everything went well! If you had any issue, please contact me.
After some testing, I noted that there might be an error during the installation if you don't have Java (please install it before!) and if your version of Java (32 bit or 64 bit) doesn't match the version of R (please, check that if the installation doesn't work!).


## Loading the package

The following line will load the package and all its dependencies in your current working environment. Let's start the fun!

```{r, message=FALSE, warning=FALSE}
library(GetFeatistics)
```


## Create mock data

In this vignette, we will use some mock data that simulate targeted and untargeted metabolomics experiments.

```{r}
create_df_examples()
```

The vignette is composed of three main chapters: **targeted analyses**, **non-targeted analyses**, and **statistical analyses**.


# Targeted analyses

To elaborate a targeted analyses, we need to organise the data in at least two tables:

1. The first should contain the intensities of peak area. Samples in rows, analysed compounds in columns. Hopefully you can export this table from your chromatographic vendor software.
Here we will be using the _df_example_targeted_:

```{r}
df_example_targeted
```

2. We need to prepare a second table in which:
 - the first column must be identical to the first column of the first table;
 - the second column should contain the following: "blank", "curve", "qc", or "unknown".
 - the third column should have the actual known values for "curve" and "qc" samples.

Not clear? Have a taste of the _df_example_targeted_legend_:

```{r}
df_example_targeted_legend
```

3. Did I say "at least" two tables, remember? Well, even though it's not mandatory, preparing a third table is useful especially if you work with internal standards. This data frame should contain four columns:
 - in the first column, the name of the targeted molecules;
 - in the second column, the matched internal standard (or NA if there isn't an internal standard for that molecule);
 - in the third column, the weighting factor you want for the calibration curve for that molecule. I highly recommend to use a weighting factor such as **1/X**, **1/Y**, or **1/(Y^2)** for a better accuracy of low concentrated analytes, but if you still don't want, you can put "none" or NA;
 - in the fourth column, the unit of measure of the known concentration (this is only for visualizations and it's not considered for any calculation)

Yes, everything as in the _df_example_targeted_compounds_legend_:

```{r}
df_example_targeted_compounds_legend
```

Now that we have all the tables, we just need to pass them as the three arguments of the _get_targeted_elaboration_ function!

```{r}
TARGETED_EXAMPLE_RESULTS <- get_targeted_elaboration(data_intensity = df_example_targeted,
                                                     data_legend = df_example_targeted_legend,
                                                     compound_legend = df_example_targeted_compounds_legend)
```


The output of the function is a list with 5 useful things:

- _results_concentrations_: a table with the calculated concentrations (you will need only this for the following statistical analyses)
- _results_accuracy_: a table with the calculated accuracy (%) for curve points and qc. It's good to check that most of these values are between 80% and 120%!
- _cv_internal_standards_: the relative standard deviation (%) of the intensities of internal standards.
- _compound_legend_: it's the same table provided in the third argument. I thought it was good to keep this valuable information!
- _summary_regression_models_: stuff of the fitted linear regression models such as: slope, intercept, r squared, and adjusted r squared.
- _regression_models_: a list with the all the fitted regression models.

Amazing! Let's check for example how the _results_concentrations_ table looks like:

```{r}
TARGETED_EXAMPLE_RESULTS$results_concentrations
```


Or maybe we want to see if the coefficient of determination is good enough, that numeric value is reported in the _summary_regression_models_ table:

```{r}
TARGETED_EXAMPLE_RESULTS$summary_regression_models
```

Well, they are calibration curves, right? So, better visualise them! Let's use the function _plot_calibration_curves_. It will create a list of ggplots that you can either print on R or export using the  _export_figures_ function.

```{r}
TARGETED_EXAMPLE_CALIBRATION_CURVES <- plot_calibration_curves(targeted_elaboration = TARGETED_EXAMPLE_RESULTS)

TARGETED_EXAMPLE_CALIBRATION_CURVES$molecule01
```

```{r, eval=FALSE}
export_figures(TARGETED_EXAMPLE_CALIBRATION_CURVES, "TARGETED_EXAMPLE_CALIBRATION_CURVES", exprt_fig_type = "pdf")
```


Last step before moving to the next chapter: let's store separately just what we will need later for the statistics. Two easy steps.

1. Collect the data frame of results of unknown samples in an object:

```{r}
TARGETED_RESULTS_FOR_STATISTICS <- filter(TARGETED_EXAMPLE_RESULTS$results_concentrations, sample_type == "unknown")
```

2. Collect the name of the variables in another distinct object:

```{r}
TARGETED_MOLECULES <- TARGETED_EXAMPLE_RESULTS$compound_legend$compounds
```


# Non-targeted analyses

This package is supposed to be used after obtaining a feature table using open source tools such as XCMS and MS-Dial.
In particular, we need a table of feature intensities like this:

```{r}
df_example_feat_intensities
```

and a featINFO table, whose first column must be identical to the first column of the feature intensity table, and the other are retention times, m/z, and any other valuable information related to that feature, such as this one:

```{r}
df_example_feat_info
```

If you worked with MS-dial, you just need to export the areas or the normalized areas, upload the .txt table on R using a function like _read_tsv()_ and then pass it to the functions _get_feat_table_from_MSDial_ and _get_feat_info_from_MSDial_. For example, something like that:

```{r, eval=FALSE}

POTENTIAL_FEAT_TABLE <- get_feat_table_from_MSDial(read_tsv("_NAME_OF_THE_EXPORTED_MSDIAL_FILE_.txt"))

POTENTIAL_FEAT_INFO <- get_feat_info_from_MSDial(read_tsv("_NAME_OF_THE_EXPORTED_MSDIAL_FILE_.txt")) %>% get_AnnoLevels_MSDial()
```

If you used XCMS while working within patRoon (and if you didn't, I suggest you to do it next time as patRoon is a great "docking station" tool that feature several widely used R-based algorithms, such as metFrag for example, check it out: https://github.com/rickhelmus/patRoon) you can use the functions _get_feat_table_from_patRoon_ and _get_feat_info_from_patRoon_. Below some examples:

```{r, eval=FALSE}

POTENTIAL_FEAT_TABLE <- get_feat_table_from_patRoon(patRoon_featureGroups = _PUT_HERE_THE_featureGroups_TABLE_FROM_patRoon_)

POTENTIAL_FEAT_INFO <- get_feat_info_from_patRoon(patRoon_featureGroups = _PUT_HERE_THE_featureGroups_TABLE_FROM_patRoon_,
                                                  patRoon_MFsummary = NULL,
                                                  MFsummary_score_columns = c("individualMoNAScore", "score"),
                                                  add_AnnoLevels = TRUE)
```

Great. The previous two chunks are not actually run, as in this vignette we will be using the mock df_examples. Speaking of which, let's look at a table that is necessary for the next step of this workflow: the filtration of features following some defined cut-offs considereing pooled QC.

- The first column are the samples,
- the second column should contain "REMOVE" (to not consider at all those samples, I suggest to do so for the first analyses of each batch), "blank", "QC", "QC_half" (if you prepared some 50:50 dilution of pooled QC samples) or "unknown".
- The third should be prepared only separated pooled QCs were prepared. If you want more information of such an approach, please check the idea behind it by reading this paper: https://doi.org/10.3390/molecules27082580.

```{r}
df_example_qc_sampletype
```


This table, together with the table of feature intensities, is all you need to run the _QCs_process_ function, which will filter out from your table the features that will not meet the cut-offs:

- _step1_: if TRUE, features with a relative standard deviation (CV%) greater than the value defined in the _step1_cutoff_ will be filtered out.
- _step2_: if TRUE, features not present in at least a percentage of QC samples as defined in _step2_cutoff_ will be filtered out.
- _step3_: if TRUE, features with a blank contribution, i.e.: the ratio between mean of blank and mean of QC, greater than the value defined in the _step3_cutoff_ will be filtered out.
- _step4_: if TRUE, features whose mean in "QC_half" samples are not between the percentage range of two values defined in _step4_cutoff_ compared to the mean of QCs will be filtered out.
- Moreover, if _sep_QC_ is TRUE, those steps will be performed separately in each QC groups, and then features that are confirmed in at least one of the groups defined in _QC_to_merge_ will be kept.

```{r}
UNTARGETED_FEAT_TABLE_QC_FILTERED <- QCs_process(featmatrix = df_example_feat_intensities,
                                                 sampletype = df_example_qc_sampletype,
                                                 sep_QC = TRUE,
                                                 QC_to_merge = c("tot", "F3A", "F3B", "F3C"),
                                                 step1 = TRUE,
                                                 step1_cutoff = 50,
                                                 step2 = TRUE,
                                                 step2_cutoff = 50,
                                                 step3 = TRUE,
                                                 step3_cutoff = 50,
                                                 step4 = FALSE,
                                                 step4_cutoff = c(20, 80),
                                                 rtrn_filtered_table = TRUE,
                                                 remove_results = TRUE,
                                                 remove_QC_and_blanks = TRUE)
```

I'd suggest also to filter the featINFO table accordingly, you could run something like this:

```{r}
UNTARGETED_FEAT_INFO_QC_FILTERED <- filter(df_example_feat_info,
                                           featname %in% UNTARGETED_FEAT_TABLE_QC_FILTERED$featname)
```


Brilliant. Last step of this chapter is to prepare what we need for the following statistical elaborations. Statistically speaking, considering the samples as observations (so as rows) and the features as variables (so as columns) is better for statistics. Luckily, I prepared a function that transpose our current feature table:

```{r}
UNTARGETED_RESULTS_FOR_STATISTICS <- transpose_feat_table(UNTARGETED_FEAT_TABLE_QC_FILTERED)
```

Finally, we also need to collect the feature names in a character vector object:

```{r}
FEATURES_CONSIDERED <- colnames(UNTARGETED_RESULTS_FOR_STATISTICS)[-1]
```


# Statistical analysis

Not bad so far. Before actually start off with the statistics, let's put together the data from targeted and untargeted analyses in a single dataframe, together also with some other variables relevant for your samples (for example in an epidemiologic study, we could have patient data such as age, BMI, sex, and more). In this vignette we will use mock data from the _df_example_sample_data_ which contains three categoriacal variables and two numerical variables.


```{r}
EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES <- df_example_sample_data %>%
  left_join(TARGETED_RESULTS_FOR_STATISTICS, by = "samples") %>%
  left_join(UNTARGETED_RESULTS_FOR_STATISTICS, by = "samples")
```


Moreover, let's put together the character vectors of the known molecule variables from the targeted analyses with the character vector of the feature names.

```{r}
ALL_POTENTIAL_COMPOUNDS <- c(TARGETED_MOLECULES, FEATURES_CONSIDERED)
``` 

Obviously, if you have only targeted or only untargeted experiments, and if you don't have any other data to join, you can skip this preparatory step and go directly on with the statistics.
 

## Descriptive statistics 

Have you ever wished of a table that gives you quick summary of the data with mean or median, and/or with percentiles? Maybe also with the option of doing it separately for each group of a defined categorical variables (that could be for example the control/disease groups)?
Your dream will come true with the _gentab_descr_ function!
Just put the dataframe in the first argument, the character vector with the name of all the numerical variables of interest in the second argument, the name of the factor variable in the third argument.
You specify what you want in the type argument, for example "median (5th; 95th percentile)". Check the full documentation by tying _?gentab_descr_ to know the other options such as setting a defined number of digital places, or how to consider missing values.


```{r}
EXAMPLE_DESCRIPTIVE_STATISTICS_RESULTS <- gentab_descr(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES,
                                                       v = ALL_POTENTIAL_COMPOUNDS,
                                                       f = "factor_condition3lev",
                                                       type = "median (5th; 95th percentile)",
                                                       ROUND = TRUE,
                                                       dig = 3,
                                                       unit_mes = NA,
                                                       missing = "only if >0")
``` 
 
Don't forget to export it as an external table with the _export_the_table_ function and it will be ready to be used as a table of your next publication!
 
```{r, eval=FALSE}
export_the_table(EXAMPLE_DESCRIPTIVE_STATISTICS_RESULTS, "EXAMPLE_DESCRIPTIVE_STATISTICS_RESULTS", "txt")
``` 

 
## Data normalisation

Before moving to parametric inferential statistics, we should check if the data are normally distributed. We could do it in three different ways:

1. We could plot a density plot and check if the data distribution resembles a Gaussian:

```{r}
DENSITY_PLOTS <- test_normality_density_plot(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES, v = ALL_POTENTIAL_COMPOUNDS)
DENSITY_PLOTS[[1]]
```
 
2. We could plot a Q–Q plot, anchd check that all points are within the grey area of the graph: 

```{r}
QQPLOT <- test_normality_q_q_plot(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES, v = ALL_POTENTIAL_COMPOUNDS)
QQPLOT[[1]]
```
 
3. We could apply Shapiro–Wilk tests: if the _P_-value is less than a defined cut-off (classical 0.05) means that data are not normally distributed:


```{r}
SHAPIRO_TABLE <- test_normality_Shapiro_table(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES, v = ALL_POTENTIAL_COMPOUNDS, pvalcutoff = 0.05, cutpval = FALSE)
SHAPIRO_TABLE
```


Most of the time, we need to normalise metabolomics data. We can do it in one step with the _transf_data_ function. Here a little guide to how to prepare the arguments:

- As most of the statistical function of the package, put the data frame of interest, and a character vector with the name of the numerical columns of interest in the first two arguments
- If _missing_replace_ is TRUE, each NA in the data will be replaced by the minimum value of that variable, after being processed by an operation ("divide", "multiply", or "exponentiate") passed to _missing_repl_type_ and the value in _missing_repl_value_. For example, if you want to replace the missing values with 1/5 of the minimum value, set _missing_repl_type = "divide"_ and _missing_repl_value = 5_. If you want to replace missing values with the squared root of the minimum value, set _missing_repl_type = "exponentiate"_ and _missing_repl_value = 1/2_.
- If _log_transf_ is TRUE, the data will be log-transformed and the base of the logarithm is the value reported in _log_base_ (choose exp(1) is for the natural logarithm).
- If _scaling_ is TRUE, data will be scaled considering what is reported in _scaling_type_, in particualr:
  - "mean_scale": data are subtracted by the mean;
  - "auto_scale": data are subtracted by the mean and divided by the standard deviation;
  - "pareto_scale": data are subtracted by the mean and divided by the squared root of the standard deviation;
  - "range_scale": data are subtracted by the mean and divided by the difference between the maximum and the minimum values.
- The output will be a table with more columns containing the transformed values, to automatically get the names of those columns in separate objects saved in the global environment, you can specify _vect_names_transf_ as TRUE and the starting part of the name that object in _name_vect_names_

Hopefully running the example will make things clear:


```{r}
EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf <-  transf_data(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES,
                                                              v = ALL_POTENTIAL_COMPOUNDS,
                                                              missing_replace = TRUE,
                                                              missing_repl_type = "divide",
                                                              missing_repl_value = 5,
                                                              log_transf = TRUE,
                                                              log_base = exp(1),
                                                              scaling = TRUE,
                                                              scaling_type = "pareto_scale",
                                                              vect_names_transf = TRUE,
                                                              name_vect_names = "ALL_POTENTIAL_COMPOUNDS_transf")
```

Look at the column names of the object: "_mr" has been added for missing value replaced, "_ln" for the log-transformation, and "paretosc" for the pareto scaling:

```{r}
glimpse(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf)
```

Those name are also saved in different objects:

```{r}
ALL_POTENTIAL_COMPOUNDS_transf_mr
ALL_POTENTIAL_COMPOUNDS_transf_mr_ln
ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc

```

These object with character vector of column names are useful to be passed to the next functions. We want to consider the data missing value replaced, log-transformed, and pareto scaled, that's why we will use the _ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc_ object.

 
## Univariate statistics

Let's compare differences in two groups with a t-test. A t-test for each numerical variable of interest will be performed with the following function.
Overall, we need to set the arguments in a similar way to other functions of this package:

- the full data frame in the first argument;
- a character vector with column names of the numerical variable of interest in the second argument;
- the name of the column with the categorical variable (since this is a t-test, it must be a categorical variable with two levels) in the third argument;
- _paired_ set to TRUE for a paired t-test;
- _FDR_ to TRUE for further correcting the p-vales for multiple corrections across the different features;
- if _cutPval_ is TRUE, every p-value below 0.001 will be replaced with "<0.001";
- if _groupdiff_ is TRUE, additional columns specifying which group has the higher mean will be also added to the table, only for significant differences considering the cutoff specified in _pcutoff_;
- if _filter_sign_ is TRUE, differences that have a p-value higher than _pcutoff_ will be filtered out.


```{r}
EXAMPLE_T_TEST <- gentab_P.t.test(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                  v = ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc,
                                  f = "factor_condition2lev",
                                  paired = FALSE,
                                  FDR = TRUE,
                                  cutPval = FALSE,
                                  groupdiff = TRUE,
                                  pcutoff = 0.05,
                                  filter_sign = FALSE)

EXAMPLE_T_TEST
```



Very similarly, we can do a one-way ANOVA, with posthocTurkeyHSD for pairwise group comparison. The argument are similar to the previous function, obviously in the third one we need to put the name of a categorical variable with three or more levels:

```{r}
EXAMPLE_ANOVA_1WAY <-  gentab_P.1wayANOVA_posthocTurkeyHSD(DF = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                                           v = ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc,
                                                           f = "factor_condition3lev",
                                                           FDR = TRUE,
                                                           groupdiff = TRUE,
                                                           pcutoff = 0.05,
                                                           filter_sign = FALSE,
                                                           cutPval = FALSE)
glimpse(EXAMPLE_ANOVA_1WAY)
```


A nice graphical visualisation of the distribution of the data can be obtained with the following function that generate some boxplots:

```{r}
EXAMPLE_BOX_PLOTS <- getBoxplots(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                 v = ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc,
                                 f = "factor_condition3lev")
EXAMPLE_BOX_PLOTS[[1]]
```


Since it generates a list with a box plot for each variable of interest. We could use the _export_figures_ to export them all in a single pdf file:

```{r, eval=FALSE}
export_figures(EXAMPLE_BOX_PLOTS, "EXAMPLE_BOX_PLOTS", exprt_fig_type = "pdf")
```



Ah! ...and why, just one-way? Two-ways ANOVA are also an option and can be obtained with the following function, specifying the two categorical factor in the third argument and if we want interactions in the _interact_ argument:

```{r}
EXAMPLE_ANOVA_2WAY <-  gentab_P.2wayANOVA_posthocTurkeyHSD(DF = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                                           v = ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc,
                                                           f = c("factor_condition3lev", "factor_condition4lev"),
                                                           interact = FALSE,
                                                           FDR = TRUE,
                                                           groupdiff = TRUE,
                                                           pcutoff = 0.05,
                                                           filter_sign = FALSE,
                                                           cutPval = FALSE)
```
 


Great, last part of this chapter: merging the statistics just obtained with the featINFO table from an untargeted study!
Afterall, the name of this package is GetFeatistics (get features + statistics!!)

You can do that with the _addINFO_to_table_ function, after that we renamed the features as they were pefore the transformatio (this, removing "_mr_ln_paretosc"):

```{r}
EXAMPLE_ANOVA_2WAY$Dependent <- str_remove_all(EXAMPLE_ANOVA_2WAY$Dependent, "_mr_ln_paretosc")

FINAL_FEAT_INFO_COMBINED_WITH_STATISTICS <- addINFO_to_table(df1 = EXAMPLE_ANOVA_2WAY,  
                                                             colfeat_df1 = "Dependent",
                                                             dfINFO = df_example_feat_info,
                                                             colfeat_dfINFO = "featname",
                                                             add_char_to_INFO = FALSE,
                                                             char_to_add_to_INFO = "_INFO")
```

The warnings that we got, tell us that the molecule of our mock target analyses did not find a match in the featINFO, and that's correct of course!

This is a glimpse of the content of this table:
```{r}
glimpse(FINAL_FEAT_INFO_COMBINED_WITH_STATISTICS)
``` 

Which we can finally export it this way:

```{r, eval=FALSE}
export_the_table(FINAL_FEAT_INFO_COMBINED_WITH_STATISTICS, "FINAL_FEAT_INFO_COMBINED_WITH_STATISTICS", "txt")
```



## Linear models
 
And now, the gem of the package and the statistical approach that I personally like the most: multiple regression linear models!!
The cool part of linear model is that they can assess the association of variables while taking into account for multiple potential confounding factors.


All in a single function, that I called: _gentab_lm_long_.
 
within the function we can distinguish three modes, specified in the _mdl_ argument:

1. "lm": linear models (with fixed effects), using the _lm_ function from the stats package.
Usually, a multivariate linear model can be built, as example, with a formula like this:
dependent_variable ~ independent_variable1 + independent_variable2 + independent_variable3

In this function, different linear models will be fitted, one for each dependent variable of interest. Usually I would consider as dependent variable each measured molecule/feature.
- As for previosu function, put the full data frame in the first argument and a character vector with all the numeric variable names to be considered as dependent variables;
- Now, pay attention to the third argument, _form_ind_: there you need to put everything you would put after the ~ of the formula. Like: "independent_variable1 + independent_variable2 + independent_variable3". They must be names of columns with numeric or factor variables in the dataframe;
- if _var_perc_ is TRUE, besides the slope, an additional column with the variation percentage will be added. The data in the dependent variables should be log-transformed and scaled to correctly do this operation. The variation percentage is calculated this way: (((base^beta)-1)*100). The base of the logaritm have to be provided in the _base_ argument (exp(1) is 2.718282... so for the natural logaritm).
- Similarly to previously described functions, _FDR_ as TRUE is to add the multiple correction to the p-value, _filter_sign_ as TRUE is to keep only results with a p-value below what we indicate in _pcutoff_ and _cutPval_ as TRUE will replace p-value<0.001 with "<0.001".


```{r}
EXAMPLE_LINEAR_MODEL <- gentab_lm_long(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                       dep = ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc,
                                       form_ind = "factor_condition2lev + factor_condition3lev + numerical_condition_a + numerical_condition_b",
                                       mdl = "lm",
                                       var_perc = TRUE,
                                       base = exp(1),
                                       FDR = TRUE,
                                       filter_sign = FALSE,
                                       pcutoff = 0.05,
                                       cutPval = FALSE)
``` 


Let's have a glimpse to the table that we got out of it:
- For each line, there are the columns _dependent_ and _independent_. Among the independents, there will be the "(Intercept)", that doesn't really have any biological significance. For categorical variables, independent will contain the name of each non-reference group. For example, the variable _factor_condition3lev_ has 3 levels: "F3A", "F3B", and "F3C"; Since "F3A" is the reference category, _independent_ will contain: "factor_condition3levF3B", which tell us about the association of the dependent variable in F3B vs F3A, and "factor_condition3levF3C", explaining what happens comparing F3C vs F3A.
- _N_observations_: is a check that tell us how many observations have been used for that linear model.
- _beta_ is the slope for that dependent and independent variables, _beta_95confint_lower_ and _beta_95confint_upper_ are the lower and upper limit of the 95% confidence interval of the beta, _SE_ is the standard error of it; and _adj_R_sqrd_ is the adjuster R squared of the model.
- _Pvalue_ and _FDR_Pvalue_ are the p-value referred to the beta being significantly different from zero.
- _negative_log10p_ and _negative_log10fdr_ are the negative log-transformed (base 10) P-values (usefull for VOlcano plots, see later) 
- _variation_perc_ is the calculated variation percentage

```{r}
glimpse(EXAMPLE_LINEAR_MODEL)
``` 


These are great data for a Volcano plot!
Considering an independent variable, a Volcano plot will allow us to see the associations with it of all the dependent variables at once:

- put the linear models table results in the first argument;
- the independent variable name in the second argument;
- in the third and fourth augment, we put what we want in the x-axis (usually "beta" or "variation_perc") and on the y-axis ("negative_log10p" or "negative_log10fdr");
- if we set _dep_cat_ as TRUE and we specify in _category_ a further column in the table with a categorization for the dependent variables (for example the class of the molecules), then the dots will be differently colored based on that categories;
- only the names of the dependent variables with an Y-valuegrater than the _cut_off_names_ will be shown (so if we only whant names of what has a P-value lower than 0.0001, we can put here -log10(0.0001));
- with _line1_ and/or _line2_ set to TRUE, additional horizontal dotted lines will be reported in the _line1_position_ and/or _line2_position_.

```{r}
EXAMPLE_VOLCANO_PLOT_1 <- Volcano_lm(tab = EXAMPLE_LINEAR_MODEL,
                                     ind_main = "numerical_condition_a",
                                     x_values = "variation_perc",
                                     y_values = "negative_log10fdr",
                                     dep_cat = FALSE,
                                     category = NULL,
                                     cut_off_names = -log10(0.0001),
                                     line1 = TRUE,
                                     line1_position = -log10(0.05),
                                     line2 = FALSE)

EXAMPLE_VOLCANO_PLOT_1
``` 

As usual, we can easily export it in this way:

```{r, eval=FALSE}
export_figures(EXAMPLE_VOLCANO_PLOT_1, "EXAMPLE_VOLCANO_PLOT_1", exprt_fig_type = "png", plot_sizes = c(14, 14), plot_unit = "in")
``` 

Pay attention: if we want to generate a Volcano plot to appreciate the association with a categorical variable, remember that the name of the independent variables includes also the group we want to check (compared to the reference group):

```{r}
EXAMPLE_VOLCANO_PLOT_2 <- Volcano_lm(tab = EXAMPLE_LINEAR_MODEL,
                                     ind_main = "factor_condition2levF2B",
                                     x_values = "variation_perc",
                                     y_values = "negative_log10fdr",
                                     dep_cat = FALSE,
                                     category = NULL,
                                     cut_off_names = -log10(1e-04),
                                     line1 = TRUE,
                                     line1_position = -log10(0.05),
                                     line2 = FALSE)

EXAMPLE_VOLCANO_PLOT_2
``` 


2. "lmer": linear models with mixed effects (random and fixed), using the _lmer_ function from the lme4 package.
These models are particularly useful if we want to include co-variates that contain observations not independently each other (we would use a random effect in that case).
For example, in a time series analyses on same patients, the variable identifying the patients should be considered as an independent variable with random effects in this way:
dependent_variable ~ independent_variable1 + independent_variable2 + (1|variable_with_random_effects)

Just as before, everything after the "~" should be passed to the third argument of the _gentab_lm_long_ function (as multiple models will be fitted, each for each dependent variable passed in the second argument):

```{r, eval = FALSE}
EXAMPLE_MIXED_LINEAR_MODEL <- gentab_lm_long(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                             dep = ALL_POTENTIAL_COMPOUNDS_transf_mr_ln_paretosc,
                                             form_ind = "factor_condition2lev + factor_condition3lev + numerical_condition_a + numerical_condition_b + (1|factor_condition4lev)",
                                             mdl = "lmer",
                                             var_perc = TRUE,
                                             base = exp(1),
                                             FDR = TRUE,
                                             filter_sign = FALSE,
                                             pcutoff = 0.05,
                                             cutPval = FALSE)
``` 
 

3. "tobit": TOBIT linear models, using the _tobit_ function of the AER package.
Particularly useful for the targeted analyses, Tobit models are useful to trad dependent variable as in between categorical and continuous variables.
In particular, if we have value below the Lower Limit Of Detection of our analytical method, we can pass those as left-censored values; and if we have values higher than the the Upper Limit Of Detection, we can specify them as right censored values. The _left_cens_ and _right_cens_ arguments are made for this: we can pass a named numeric vector with those values (the names of the vector must be the dependent variables).
Of course if we transformed the data, we shoudl transforme these data as well:

```{r, eval = FALSE}

LOD_molecules_ln_paretosc <- c(molecule01_mr_ln_paretosc = (log(10)-mean(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule01_mr_ln))/sqrt(sd(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule01_mr_ln)),
                               molecule02_mr_ln_paretosc = (log(100)-mean(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule02_mr_ln))/sqrt(sd(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule02_mr_ln)),
                               molecule03_mr_ln_paretosc = (log(1)-mean(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule03_mr_ln))/sqrt(sd(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule03_mr_ln)),
                               molecule04_mr_ln_paretosc = (log(10)-mean(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule04_mr_ln))/sqrt(sd(EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf$molecule04_mr_ln)))

EXAMPLE_TOBIT_LINEAR_MODEL <- gentab_lm_long(df = EXAMPLE_TABLE_FOR_STATISTICAL_ANALYSES_transf,
                                             dep = c("molecule01_mr_ln_paretosc" "molecule02_mr_ln_paretosc" "molecule03_mr_ln_paretosc" "molecule04_mr_ln_paretosc"),
                                             form_ind = "factor_condition2lev + factor_condition3lev + numerical_condition_a + numerical_condition_b",
                                             mdl = "tobit",
                                             left_cens = LOD_molecules_ln_paretosc,
                                             right_cens = NULL,
                                             var_perc = TRUE,
                                             base = exp(1),
                                             FDR = TRUE,
                                             filter_sign = FALSE,
                                             pcutoff = 0.05,
                                             cutPval = FALSE)
``` 
 
 

## Conclusion

I hope you enjoying the GetFeatistics package!
Don't forget to check the full documentation for each function.
And don't forget to cite it if you use if for your elaborations!
Finally, thank you for reporting me any issue or bug you might encounter while using this package!
